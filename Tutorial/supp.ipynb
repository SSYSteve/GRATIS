{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "In this section, we briefly introduce the basic concepts of graph representation as well as the general vertex and edge updating mechanism of Graph Neural Networks (GNNs).\n",
    "\n",
    "## Graph Representation\n",
    "\n",
    "A graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ is composed of a set of vertices $\\mathcal{V} \\subseteq \\{\\mathbf{v}_i \\in \\mathbb{R}^{1 \\times K}\\}$, and edges $\\mathcal{E} \\subseteq \\{\\mathbf{e}_{i,j} = \\mathbf{e}(\\mathbf{v}_i, \\mathbf{v}_j) \\mid \\mathbf{v}_i, \\mathbf{v}_j \\in \\mathcal{V},  i \\neq j\\}$. Here, $\\mathbf{v}_i$ represents $K$ attributes of the $i_{th}$ object/component in the predefined graph or non-graph data sample, and $\\mathbf{e}_{i,j}$ represents the edge feature that defines the relationship between vertices $\\mathbf{v}_i$ and $\\mathbf{v}_j$. Each pair of vertices can be connected by at most one undirected edge or two directed edges.\n",
    "\n",
    "A standard way to describe such edges is through the adjacency matrix $\\mathcal{A} \\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$, where all vertices in a graph are ordered so that each vertex indexes a specific row and column. The presence of each edge $\\mathbf{e}_{i,j}$ is described by a binary value $\\mathcal{A}_{i,j} = 1$ if $\\mathbf{v}_i$ and $\\mathbf{v}_j$ are connected, or $\\mathcal{A}_{i,j} = 0$ otherwise. The adjacency matrix is symmetric if all edges are undirected but can be non-symmetric if one or more directed edges exist. Instead of using a binary value, some studies also build adjacency matrices with continuous real values to describe the strength of association between each pair of vertices.\n",
    "\n",
    "## Vertex/Edge Updating of Message-Passing GNNs\n",
    "\n",
    "Recently, message-passing Graph Neural Networks (GNNs), including Graph Convolution Networks (GCNs), have become dominant models for a wide variety of graph analysis tasks. Given a GNN $G$, its $l_{th}$ layer $G^l$ takes the graph $\\mathcal{G}^{l-1} = (\\mathcal{V}^{l-1}, \\mathcal{E}^{l-1})$ produced by the $(l-1)_{th}$ layer $G^{l-1}$ as input and generates a new graph $\\mathcal{G}^{l} = (\\mathcal{V}^l, \\mathcal{E}^l)$, which can be formulated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{G}^{l} = G^l(\\mathcal{G}^{l-1})\n",
    "$$\n",
    "\n",
    "Specifically, the vertex feature $\\mathbf{v}^l_i$ in $\\mathcal{G}^{l}$ is computed based on: \n",
    "1. Its previous status $\\mathbf{v}_i^{l-1}$ in $\\mathcal{G}^{l-1}$.\n",
    "2. A set of adjacent vertices $\\mathbf{v}_j^{l-1} \\subseteq \\mathcal{N}(\\mathbf{v}_i^{l-1})$ in $\\mathcal{G}^{l-1}$, where $\\mathcal{A}^{l-1}_{i,j} = 1$, and $\\mathcal{A}^{l-1}$ is the adjacency matrix of $\\mathcal{G}^{l-1}$.\n",
    "3. A set of edge features $\\mathbf{e}_{j,i}^{l-1}$ that represent the relationship between every $\\mathbf{v}_j^{l-1}$ and $\\mathbf{v}_i^{l-1}$ in $\\mathcal{N}(\\mathbf{v}_i^{l-1})$.\n",
    "\n",
    "Here, the message $\\mathbf{m}_{\\mathcal{N}(\\mathbf{v}^{l-1}_i)}$ is produced by aggregating all adjacent vertices of $\\mathbf{v}_i^{l-1}$ through related edges $\\mathbf{e}_{j,i}^{l-1}$, which can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{m}_{\\mathcal{N}(\\mathbf{v}^{l-1}_i)} &= M\\left(\\mathbin\\Vert ^{N}_{j=1} f(\\mathbf{v}_j^{l-1}, \\mathbf{e}_{j,i}^{l-1})\\right) \\\\\n",
    "\n",
    "f(\\mathbf{v}_j^{l-1}, \\mathbf{e}_{j,i}^{l-1}) &= 0 \\quad \\text{subject to} \\quad \\mathcal{A}^{l-1}_{i,j} = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $M$ is a differentiable function that aggregates messages produced from all adjacent vertices; $N$ denotes the number of vertices in the graph $\\mathcal{G}^{l-1}$; $f(\\mathbf{v}_j^{l-1}, \\mathbf{e}_{j,i}^{l-1})$ is a differentiable function defining the influence of an adjacent vertex $\\mathbf{v}_j^{l-1}$ on the vertex $\\mathbf{v}_i^{l-1}$ through their edge $\\mathbf{e}_{j,i}^{l-1}$; and $\\mathbin\\Vert$ is the aggregation operator to combine messages of all adjacent vertices of $\\mathbf{v}_i^{l-1}$. As a result, the vertex feature $\\mathbf{v}^l_i$ can be updated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^l_i = G_v^l(\\mathbf{v}^{l-1}_i, \\mathbf{m}_{\\mathcal{N}(\\mathbf{v}^{l-1}_i)})\n",
    "$$\n",
    "\n",
    "where $G_v^l$ denotes a differentiable function of the $l_{th}$ GNN layer, which updates all vertex features for producing the graph $\\mathcal{G}^{l}$.\n",
    "\n",
    "Meanwhile, each edge feature $\\mathbf{e}_{i,j}^l$ in the graph $\\mathcal{G}^{l}$ can be either kept the same as its previous status $\\mathbf{e}_{i,j}^{l-1}$ in the graph $\\mathcal{G}^{l-1}$ (denoted as the GNN type 1) or updated (denoted as the GNN type 2) during GNNs' propagation. Specifically, each edge feature $\\mathbf{e}_{i,j}^l \\in \\mathcal{G}^{l}$ is computed based on:\n",
    "\n",
    "1. Its previous status $\\mathbf{e}^{l-1}_{i,j} \\in \\mathcal{G}^{l-1}$.\n",
    "2. The corresponding vertex features $\\mathbf{v}^{l-1}_i$ and $\\mathbf{v}^{l-1}_j$ in $\\mathcal{G}^{l-1}$.\n",
    "\n",
    "Mathematically, $\\mathbf{e}^{l}_{i,j}$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_{i,j}^l = \n",
    "\\begin{cases}\n",
    "\\mathbf{e}^{l-1}_{i,j} &  \\text{GNN type 1} \\\\\n",
    "G_e^l(\\mathbf{e}^{l-1}_{i,j}, g(\\mathbf{v}^{l-1}_i, \\mathbf{v}^{l-1}_j)) & \\text{GNN type 2}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $G_e^l$ is a differentiable function of the $l_{th}$ GNN layer, which updates edge features to produce the graph $\\mathcal{G}^{l}$, and $g$ is a differentiable function that models the relationship between $\\mathbf{v}^{l-1}_i$ and $\\mathbf{v}^{l-1}_j$. In summary, vertex and edge features' updating are mutually influenced during the propagation of message-passing GNNs. Please refer to Hamilton et al. and Dwivedi et al. for more details.\n",
    "\n",
    "# Methodology\n",
    "\n",
    "## Algorithmic Definitions\n",
    "\n",
    "**Task-specific:** The term refers to features or topology specifically extracted for a particular task. These are produced from the original input data using networks trained with the target labels. In simpler terms, the networks define the features or topology based on the relationship between the input and the label.\n",
    "\n",
    "**Plug-and-play:** The term refers to the ability to easily integrate the model into a machine learning framework (i.e., Backbone-GNN frameworks in this paper) without requiring significant modifications or customization.\n",
    "\n",
    "**Edge feature:** The term refers to a vector or a single value attached to a graph edge, which describes the characteristics of the edge based on the relationship between the corresponding vertex pair. In this paper, the term 'multi-dimensional edge feature' represents a vector consisting of multiple attributes to describe the characteristics of the corresponding edge.\n",
    "\n",
    "## Visualization of GD and TTP Modules\n",
    "\n",
    "Figure 1 illustrates the GD and TTP modules of the graph-based GRATIS framework. In this setup, the GCN in the backbone first produces a set of vertex features from the input graph $\\mathcal{D}^{\\text{in}}$, which are concatenated into a matrix $X^{\\text{GCN}}$. Then, the CNN processes this matrix to yield the global contextual representation $X$. The GD module treats the original input graph as the basic graph. Subsequently, the TTP module generates an adjacency probability matrix $\\mathcal{\\hat{A}}^{\\text{prob}}$ from $X$, which is combined with the basic graph topology to produce the final task-specific adjacency matrix $\\mathcal{\\hat{A}}$.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"submodules_predefined_graphs.png\" alt=\"GCN-CNN Backbone, Graph Definition (GD), and Task-specific Topology Prediction (TTP) modules for processing pre-defined graphs\" width=\"60%\" />\n",
    "</div>\n",
    "\n",
    "*Figure 1: Illustration of the GCN-CNN Backbone, Graph Definition (GD), and Task-specific Topology Prediction (TTP) modules for processing **pre-defined graphs**.*\n",
    "\n",
    "Figure 2 illustrates the GD and TTP modules for the non-graph-based GRATIS framework, which can handle various types of non-graph data (e.g., image, audio, video, text, and time-series). During training, in the GD module, the outputs of the Vertex Feature Extractor (VFE) are fed to a standard MLP-based predictor, allowing intermediate supervision of the VFE. This supervision enables the trained VFE to provide a set of task-related vertex features, which define the basic graph. Then, the TTP module jointly trains the VFE with a GCN, enabling a refined VFE ($\\hat{\\text{VFE}}$) to produce task-specific graph vertex features and a vertex-decided graph topology. These are then combined with the basic graph topology to generate the graph $\\mathcal{G}^{\\text{V}}(\\mathcal{\\hat{V}}, \\mathcal{E}^{V})$ with an optimal task-specific topology.\n",
    "\n",
    "![GD and TTP modules for **non-graph data**](submodules_non_graphs.png)\n",
    "*Figure 2: Illustration of the GD and TTP modules for **non-graph data**, which are jointly optimized during training.*\n",
    "\n",
    "## Framework differentiation analysis\n",
    "\n",
    "In this section, we show that the three modules (GD, TTP, and MEFG) of the proposed framework can be jointly trained with any differentiable backbone and GNN predictor in an end-to-end manner. This allows the weights learned for all modules to be task-specific.\n",
    "\n",
    "Firstly, the GD module for processing graph data only contains an identity mapping, while the GD module for processing non-graph data is made up of a set of differentiable Fully Connected (FC) and Global Average Pooling (GAP) layers (i.e., the VFE). Therefore, the GD module for processing both graph and non-graph data is fully differentiable. Let $\\mathcal{L}$ be a differentiable objective function that measures the loss of the final prediction generated by the GNN predictor. Its gradient with respect to the graph $\\mathcal{G}^{B}(\\mathcal{V}, \\mathcal{E})$ generated by the GD module can be computed as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{G}^{B}} &= \\frac{\\partial \\mathcal{L}}{\\partial p_{\\mathcal{\\hat{G}}}} \\frac{\\partial p_{\\mathcal{\\hat{G}}}}{\\partial \\mathcal{\\hat{G}}} \\frac{\\partial \\mathcal{\\hat{G}}}{\\partial \\mathcal{G}^{B}} \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial p_{\\mathcal{\\hat{G}}}} \\frac{\\partial p_{\\mathcal{\\hat{G}}}}{\\partial \\mathcal{\\hat{G}}} \\left(\\frac{\\partial \\mathcal{\\hat{V}}}{\\partial \\mathcal{V}}+\\frac{\\partial \\mathcal{\\hat{V}}}{\\partial X}, \\frac{\\partial \\mathcal{\\hat{E}}}{\\partial \\mathcal{V}}+\\frac{\\partial \\mathcal{\\hat{E}}}{\\partial X}\\right) \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial p_{\\mathcal{\\hat{G}}}} \\frac{\\partial \\text{GNN}(\\mathcal{\\hat{G}})}{\\partial \\mathcal{\\hat{G}}} \\left(\\frac{\\partial \\text{TTP}(X, \\mathcal{A}, \\mathcal{V})}{\\partial \\mathcal{V}} + \\frac{\\partial \\text{TTP}(X, \\mathcal{A}, \\mathcal{V})}{\\partial X}, \\frac{\\partial \\text{MEFG}(X, \\mathcal{V})}{\\partial \\mathcal{V}}+\\frac{\\partial \\text{MEFG}(X, \\mathcal{V})}{\\partial X}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p_{\\mathcal{\\hat{G}}}$ denotes the graph analysis prediction obtained from the differentiable GNN predictor; $\\mathcal{\\hat{G}}(\\mathcal{\\hat{V}}, \\mathcal{\\hat{E}})$ is the final produced task-specific graph representation ($\\mathcal{\\hat{E}}$ is produced from the MEFG module and $\\mathcal{\\hat{V}}$ is produced from the TTP module). Here, both GNN and MEFG are made up of fully differentiable layers. In other words, the terms $\\frac{\\partial \\mathcal{L}}{\\partial p_{\\mathcal{\\hat{G}}}}$, $\\frac{\\partial \\text{GNN}(\\mathcal{\\hat{G}})}{\\partial \\mathcal{\\hat{G}}}$, $\\frac{\\partial \\text{MEFG}(X, \\mathcal{V})}{\\partial \\mathcal{V}}$, and $\\frac{\\partial \\text{MEFG}(X, \\mathcal{V})}{\\partial X}$ are differentiable.\n",
    "\n",
    "In addition, the generation of $\\mathcal{\\hat{A}}^{\\text{prob}}$ for graph data and $\\mathcal{\\hat{A}}^{\\text{V}}$ for non-graph data is fully decided by global contextual representation $X$ and task-specific vertex features $\\mathcal{\\hat{V}}$, both of which are produced via fully differentiable operations. Specifically, $X$ is generated by a CNN/GCN backbone, and $\\mathcal{\\hat{V}}$ is produced by either identity mapping for predefined graphs or the VFE for non-graph data. As a result, the gradients for updating $\\mathcal{A}$ to $\\mathcal{\\hat{A}}^{\\text{prob}}$ or $\\mathcal{\\hat{A}}^{\\text{V}}$ can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{A}} \n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{V}}+\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial X} \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial p_{\\mathcal{\\hat{G}}}} \n",
    "\\frac{\\partial p_{\\mathcal{\\hat{G}}}}{\\partial \\mathcal{\\hat{G}}} \n",
    "\\left(\\frac{\\partial \\mathcal{\\hat{G}}}{\\partial \\mathcal{V}} + \n",
    "\\frac{\\partial \\mathcal{\\hat{G}}}{\\partial X} \\right) \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial p_{\\mathcal{\\hat{G}}}} \n",
    "\\frac{\\partial p_{\\mathcal{\\hat{G}}}}{\\partial \\mathcal{\\hat{G}}} \n",
    "\\left(\\frac{\\partial \\mathcal{\\hat{V}}}{\\partial \\mathcal{V}} + \n",
    "\\frac{\\partial \\mathcal{\\hat{E}}}{\\partial \\mathcal{V}} + \n",
    "\\frac{\\partial \\mathcal{\\hat{V}}}{\\partial X} + \n",
    "\\frac{\\partial \\mathcal{\\hat{E}}}{\\partial X}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "As analyzed in Equation \\ref{eq:back-propagation}, all terms in Equation \\ref{eq:back-propagation_A} are differentiable, which means that the back-propagated gradients would enforce the adjacency matrices $\\mathcal{\\hat{A}}^{\\text{prob}}$ or $\\mathcal{\\hat{A}}^{\\text{V}}$ to be also learned in an end-to-end manner. Although the process that updates the adjacency matrix from $\\mathcal{A}$ to $\\mathcal{\\hat{A}}$ (Equation (19) of the main document) in the TTP module is not fully differentiable, to the best of our knowledge, existing GNNs do not forward the adjacency matrix within the network. That is, Equation \\ref{eq:back-propagation} does not need to compute $\\frac{\\partial \\text{TTP}(X, \\mathcal{A}, \\mathcal{V})}{\\partial A}$. Consequently, the proposed approach is a fully differentiable framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
